{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-30T08:37:05.154942420Z",
     "start_time": "2023-08-30T08:37:04.604886159Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import EyeTrackingDataLoader\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Script parameters\n",
    "- Language: supported 'it' and 'en'\n",
    "- Model name: the model to use\n",
    "- Rollout: if to apply rollout to ValueZeroing\n",
    "- Aggregation method: how to compute the attention of a word composed of multiple sub-tokens. Supported values:\n",
    "\t- sum: sum attention values of all sub-tokens\n",
    "\t- max: take the maximum attention value among sub-tokens\n",
    "\t- mean: average the attention values of the sub-tokens\n",
    "\t- first: take the attention value of the first sub-token\n",
    "- Layer: the layer from which to extract attention"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea696bc14c127804"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "language = 'en'\n",
    "rollout = True\n",
    "aggregation_method = 'sum'\n",
    "layer = 11\n",
    "model_name = 'xlm-roberta-base'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T08:37:05.156778606Z",
     "start_time": "2023-08-30T08:37:05.154005940Z"
    }
   },
   "id": "97df8eda542447e3"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'data/value_zeroing/en/xlm-roberta-base/sum_rollout_l11.json'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eye_tracking_data_dir = f'../augmenting_nlms_meco_data/{language}'\n",
    "\n",
    "out_dir = f'data/value_zeroing/{language}/{model_name}'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "out_file_name = f'{aggregation_method}' if not rollout else f'{aggregation_method}_rollout'\n",
    "out_file_name += f'_l{layer}.json'\n",
    "out_path = os.path.join(out_dir, out_file_name)\n",
    "out_path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T08:37:05.189305920Z",
     "start_time": "2023-08-30T08:37:05.157914671Z"
    }
   },
   "id": "4d6fb36e1dd97d74"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "dl = EyeTrackingDataLoader(eye_tracking_data_dir)\n",
    "sentences_df = dl.load_sentences()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T08:37:09.920863853Z",
     "start_time": "2023-08-30T08:37:09.877932514Z"
    }
   },
   "id": "4e29c32e2be2e855"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "    sent_id                                           sentence\n0       1_1  [In, ancient, Roman, religion, and, myth,, Jan...\n1       1_2  [He, has, a, double, nature, and, is, usually,...\n2       1_3  [Janus, presided, over, the, beginning, and, e...\n3       1_4  [The, doors, of, his, temple, were, open, in, ...\n4       1_5  [As, the, god, of, gates,, he, was, also, asso...\n..      ...                                                ...\n96     12_4  [The, registration, identifier, is, a, series,...\n97     12_5  [In, some, countries,, the, identifier, is, un...\n98     12_6  [France, was, the, first, country, to, introdu...\n99     12_7  [Early, twentieth, century, plates, varied, in...\n100    12_8  [Standardization, of, plates, came, in, the, l...\n\n[101 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sent_id</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1_1</td>\n      <td>[In, ancient, Roman, religion, and, myth,, Jan...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1_2</td>\n      <td>[He, has, a, double, nature, and, is, usually,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1_3</td>\n      <td>[Janus, presided, over, the, beginning, and, e...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1_4</td>\n      <td>[The, doors, of, his, temple, were, open, in, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1_5</td>\n      <td>[As, the, god, of, gates,, he, was, also, asso...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>12_4</td>\n      <td>[The, registration, identifier, is, a, series,...</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>12_5</td>\n      <td>[In, some, countries,, the, identifier, is, un...</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>12_6</td>\n      <td>[France, was, the, first, country, to, introdu...</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>12_7</td>\n      <td>[Early, twentieth, century, plates, varied, in...</td>\n    </tr>\n    <tr>\n      <th>100</th>\n      <td>12_8</td>\n      <td>[Standardization, of, plates, came, in, the, l...</td>\n    </tr>\n  </tbody>\n</table>\n<p>101 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-30T08:37:11.870242125Z",
     "start_time": "2023-08-30T08:37:11.861243718Z"
    }
   },
   "id": "52bd9c155f63e68c"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "subword_prefix = '▁'  # non ho trovato un modo di prenderlo automaticamente dal tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T09:49:02.686974943Z",
     "start_time": "2023-08-11T09:49:01.273873348Z"
    }
   },
   "id": "2c777aa178e4a88e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create a mapping of indices to align model's tokens and sub-tokens to original words"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f19afe364b7a0f2a"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def align_to_original_words(model_tokens:list , original_tokens:list, subword_prefix:str) -> list:\n",
    "    model_tokens = model_tokens[1: -1]                  # Remove <s> and </s>\n",
    "    aligned_model_tokens = []\n",
    "    alignment_ids = []\n",
    "    alignment_id = -1\n",
    "    orig_idx = 0\n",
    "    for token in model_tokens:\n",
    "        alignment_id += 1\n",
    "        if token.startswith(subword_prefix):            # Remove the sub-word prefix\n",
    "            token = token[len(subword_prefix):]         \n",
    "        if len(aligned_model_tokens) == 0:              # First token (serve?)\n",
    "            aligned_model_tokens.append(token)\n",
    "        elif aligned_model_tokens[-1] + token in original_tokens[orig_idx]: # We are in the second (third, fourth, ...) sub-token\n",
    "            aligned_model_tokens[-1] += token                               # so we merge the token with its preceding(s)\n",
    "            alignment_id -= 1\n",
    "        else:\n",
    "            aligned_model_tokens.append(token)\n",
    "        if aligned_model_tokens[-1] == original_tokens[orig_idx]:   # A token was equal to an entire original word or a set of \n",
    "            orig_idx += 1                                           # sub-tokens was merged and matched an original word\n",
    "        alignment_ids.append(alignment_id)\n",
    "\n",
    "    if aligned_model_tokens != original_tokens:\n",
    "        raise Exception(f'Failed to align tokens.\\nOriginal tokens: {original_tokens}\\nObtained alignment: {aligned_model_tokens}')\n",
    "    return alignment_ids"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T10:20:57.705151843Z",
     "start_time": "2023-08-11T10:20:57.658175042Z"
    }
   },
   "id": "1bdd19e53e647a0"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "def create_subwords_alignment(sentences_df: pd.DataFrame, tokenizer: AutoTokenizer, subword_prefix: str) -> dict:\n",
    "    sentence_alignment_dict = dict()\n",
    "\n",
    "    for idx, row in sentences_df.iterrows():\n",
    "        sent_id = row['sent_id']\n",
    "        sentence = row['sentence']\n",
    "        tokenized_sentence = tokenizer(sentence, is_split_into_words=True, return_tensors='pt')\n",
    "        input_ids = tokenized_sentence['input_ids'].tolist()[0] # 0 because the batch_size is 1\n",
    "        model_tokens = tokenizer.convert_ids_to_tokens(input_ids) \n",
    "        aligned_tokens, alignment_ids = align_to_original_words(model_tokens, sentence, subword_prefix)\n",
    "        sentence_alignment_dict[sent_id] = {'model_input': tokenized_sentence, 'alignment_ids': alignment_ids}\n",
    "    return sentence_alignment_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T10:25:02.002820058Z",
     "start_time": "2023-08-11T10:25:01.962163249Z"
    }
   },
   "id": "af79396bb0ee48be"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "sentence_alignment_dict = create_subwords_alignment(sentences_df, tokenizer, subword_prefix)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-11T10:25:13.126867580Z",
     "start_time": "2023-08-11T10:25:13.084046668Z"
    }
   },
   "id": "d45805c8c08c2569"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "class TokenContributionExtractor():\n",
    "    \n",
    "    def __init__(self, model_name:str, layer:int, rollout:bool, aggregation_method:str):\n",
    "        self.layer = layer\n",
    "        self.rollout = rollout\n",
    "        self.aggregration_method = aggregation_method\n",
    "        self.model = self._load_model(model_name)\n",
    "    \n",
    "    def _load_model(self, model_name:str):\n",
    "        \"\"\"\n",
    "        Left abstract since some contribution extraction methods have to modify the default model architecture\n",
    "        \"\"\"\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "105f0aee1585d75b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
