{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2358d3ae-4dd2-4f81-af00-26622e591a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "928e7ca0-4a86-4b87-b6fc-b1a3f196e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from anm.utils import Config, LOGGER\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import torch\n",
    "import csv\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1da2d20-6ef5-4b65-9b4a-7d3c4589b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "312a324a-850a-4c47-9cfa-0e75c48a958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_from_faulty_csv(src_path, label):\n",
    "    dataset_dict = {'text': [], 'label': []}\n",
    "    with open(src_path) as src_file:\n",
    "        csv_reader = csv.reader(src_file, delimiter=',', quotechar='\"')\n",
    "        print('')\n",
    "        for row in csv_reader:\n",
    "            if row[0] == 'idtwitter':\n",
    "                continue\n",
    "            if len(row) != 9:\n",
    "                cut_row = row[:9]\n",
    "                cut_row[8] += ',' + ', '.join(row[9:])\n",
    "                row = cut_row\n",
    "            dataset_dict['text'].append(row[8])\n",
    "            if label == 'pos':\n",
    "                dataset_dict['label'].append(int(row[2]))\n",
    "            else:\n",
    "                dataset_dict['label'].append(int(row[3]))\n",
    "    return Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6203970-2f63-4768-84ed-c3c2692b295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(dataset_dir, tokenizer, label):\n",
    "    sentipolc_files = {\n",
    "        'train': [os.path.join(dataset_dir, file_name) for file_name in os.listdir(dataset_dir) if\n",
    "                  'training_set' in file_name][0],\n",
    "        'test':\n",
    "            [os.path.join(dataset_dir, file_name) for file_name in os.listdir(dataset_dir) if 'test_set' in file_name][\n",
    "                0]\n",
    "    }\n",
    "    train_dataset = create_dataset_from_faulty_csv(sentipolc_files['train'], label)\n",
    "    test_dataset = create_dataset_from_faulty_csv(sentipolc_files['test'], label)\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        result = tokenizer(examples[\"text\"], truncation=True)\n",
    "        return result\n",
    "\n",
    "    tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=['text'])\n",
    "    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True, remove_columns=['text'])\n",
    "\n",
    "    return tokenized_train_dataset, tokenized_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3712fd80-ffff-47d3-94e8-0a9a5b4c8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cc729f9-c2d2-423e-9256-9f19a2ae8f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_hf(model_name, pretrained):\n",
    "    # Model\n",
    "    LOGGER.info(\"Initiating model ...\")\n",
    "    if not pretrained:\n",
    "        # initiate model with random weights\n",
    "        LOGGER.info(\"Take randomized model\")\n",
    "        \n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_config(config)\n",
    "    else:\n",
    "        LOGGER.info(\"Take pretrained model\")\n",
    "    \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9b421af-80d9-4175-9b25-209ffd365911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.weight_decay = 1e-2\n",
    "        self.lr = 2e-5 #5e-6\n",
    "        self.train_bs = 8\n",
    "        self.eval_bs = 8\n",
    "        self.n_epochs = 8\n",
    "        self.seed = 1234\n",
    "        self.num_warmup_steps = 0\n",
    "        # self.language_mode = args.language_mode\n",
    "        # self.pretrained = args.pretrained\n",
    "        # self.finetuned = args.finetuned\n",
    "        # self.user_id = args.user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adea4f96-a17e-4285-819c-6b1df98ba5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8211616f-0317-41b0-a82a-34819e1ee760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"idb-ita/gilberto-uncased-from-camembert\"\n",
    "model_name = 'xlm-roberta-base'\n",
    "dataset_dir = '../augmenting_nlms_meco_data/sentiment/it_sentipolc'\n",
    "output_dir = 'xlm_p_np'\n",
    "finetuned = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "950ba9d2-f948-41a0-8214-d6d96b88dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2715f7c2-03d0-4a0d-b163-296f54c3a164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3244621e7c4083a5e3ae66ec42768a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7410 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02db1bf3ee0c4c1a8cc854b23d5abd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset, test_dataset = prepare_datasets(dataset_dir, tokenizer, 'pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1608a695-3c04-4465-a9a8-8c7ae69b99b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 7410\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d79b4a3-eb7d-4c36-ada9-2872b3330086",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "        output_dir=output_dir,          # output directory\n",
    "        num_train_epochs=cf.n_epochs,              # total number of training epochs\n",
    "        per_device_train_batch_size=cf.train_bs,  # batch size per device during training\n",
    "#        per_device_eval_batch_size=cf.eval_bs,   # batch size for evaluation\n",
    "        warmup_steps=0,#500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=cf.weight_decay,               # strength of weight decay\n",
    "        save_strategy=\"no\",\n",
    "        learning_rate=cf.lr,\n",
    "        logging_strategy='steps',\n",
    "        logging_steps=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9116f97-af7a-4ae8-ac5d-c9b3a86f2143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-12 12:10:24,410 - processing - INFO] Model retrieving, not finetuned, from hf...\n",
      "[2023-10-12 12:10:24,411 - processing - INFO] Initiating model ...\n",
      "[2023-10-12 12:10:24,411 - processing - INFO] Take pretrained model\n"
     ]
    }
   ],
   "source": [
    "if not finetuned: # downaload from huggingface\n",
    "    LOGGER.info(\"Model retrieving, not finetuned, from hf...\")\n",
    "    model = load_model_from_hf(model_name, True)\n",
    "else: # the finetuned model has to be loaded from disk\n",
    "    LOGGER.info(\"Model retrieving, finetuned, load from disk...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.model_dir, \n",
    "                                                                   ignore_mismatched_sizes=True,\n",
    "                                                                   output_attentions=False, output_hidden_states=False,\n",
    "                                                                   num_labels=2) # number of the classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "535cf17a-bbbe-4d06-833d-bf64bd1225ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "        model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "        args=training_args,                  # training arguments, defined above\n",
    "        train_dataset=train_dataset,         # training dataset\n",
    "#         eval_dataset=tokenized_datasets_sst2[\"validation\"],            # evaluation dataset\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd7fa438-6357-43f9-ab47-431ecdd4fe54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/Workspace/augmenting_nlms_meco/venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='3712' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   8/3712 00:03 < 37:51, 1.63 it/s, Epoch 0.02/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/augmenting_nlms_meco/venv/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/augmenting_nlms_meco/venv/lib/python3.10/site-packages/transformers/trainer.py:1842\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1836\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1837\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1839\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1840\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1841\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1842\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1843\u001b[0m ):\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1845\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553cd651-044c-4262-9174-842878963fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1619d111-d224-4d0c-9c7c-ab5f4e8be1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/Workspace/augmenting_nlms_meco/venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "train_metrics = trainer.evaluate(eval_dataset=train_dataset, metric_key_prefix=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0116f329-f699-457d-91f2-6ed7d984f01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        8.0\n",
      "  train_accuracy           =     0.9866\n",
      "  train_f1                 =     0.9867\n",
      "  train_loss               =     0.0626\n",
      "  train_precision          =     0.9867\n",
      "  train_recall             =     0.9866\n",
      "  train_runtime            = 0:00:44.49\n",
      "  train_samples_per_second =    166.542\n",
      "  train_steps_per_second   =     10.429\n"
     ]
    }
   ],
   "source": [
    "trainer.log_metrics(\"train\", train_metrics)\n",
    "trainer.save_metrics(\"train\", train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d066f8bd-ad0a-444a-84f9-4bcd7e89410e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luca/Workspace/augmenting_nlms_meco/venv/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer.evaluate(eval_dataset=test_dataset, metric_key_prefix=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "431b7218-c145-4f8d-92f8-42d807e37bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** test metrics *****\n",
      "  epoch                   =        8.0\n",
      "  test_accuracy           =     0.7713\n",
      "  test_f1                 =     0.7897\n",
      "  test_loss               =      1.092\n",
      "  test_precision          =     0.8243\n",
      "  test_recall             =     0.7713\n",
      "  test_runtime            = 0:00:11.99\n",
      "  test_samples_per_second =    166.535\n",
      "  test_steps_per_second   =     10.419\n"
     ]
    }
   ],
   "source": [
    "trainer.log_metrics(\"test\", test_metrics)\n",
    "trainer.save_metrics(\"test\", test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c4059f-4388-4d5e-ac39-7d2d117be1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea5e8b7-534a-4c53-8372-f023f4b8010a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3288d7-8a9b-4679-bf48-69bcf3ed6fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe9cf0-83e6-4679-96bd-c42aeac0d894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5390a4-ebce-4374-afcb-711e39a03bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9fc71-2c03-4442-86c9-166c002154d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
