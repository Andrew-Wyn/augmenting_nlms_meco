{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "129fc4c5-759e-4e13-a0dd-f64c83ce7cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from anm.gaze_dataloader.dataset import _create_senteces_from_data\n",
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d8e8e6-bb05-402c-940b-a9f4aeea866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'it'\n",
    "rollout = True\n",
    "aggregation_method = 'first'\n",
    "layer = 11\n",
    "model_name = 'xlm-roberta-base'\n",
    "subword_prefix = '▁' #'Ġ' \n",
    "method = 'attention'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acb9890f-4d0c-4fc0-90c8-1e7757379ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../augmenting_nlms_meco_data/en/en_49_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94d638cd-83da-4597-be5b-a2f944e55cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(dataset_path, index_col=0)\n",
    "gaze_dataset = _create_senteces_from_data(data, [], keep_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af588255-7a24-4c06-9486-26239e43e62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text'],\n",
       "    num_rows: 47\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaze_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46e96f0a-6967-4bdf-9659-8575464593da",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2276857e-53da-46ff-9db3-5fe4c0747684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_to_original_words(model_tokens: list, original_tokens: list, subword_prefix: str,\n",
    "                            lowercase: bool = False) -> list:\n",
    "    if lowercase:\n",
    "        original_tokens = [tok.lower() for tok in original_tokens]\n",
    "    model_tokens = model_tokens[1: -1]  # Remove <s> and </s>\n",
    "    aligned_model_tokens = []\n",
    "    alignment_ids = []\n",
    "    alignment_id = -1\n",
    "    orig_idx = 0\n",
    "    for token in model_tokens:\n",
    "        alignment_id += 1\n",
    "        if token.startswith(subword_prefix):  # Remove the sub-word prefix\n",
    "            token = token[len(subword_prefix):]\n",
    "        if len(aligned_model_tokens) == 0:  # First token (serve?)\n",
    "            aligned_model_tokens.append(token)\n",
    "        elif aligned_model_tokens[-1] + token in original_tokens[\n",
    "            orig_idx]:  # We are in the second (third, fourth, ...) sub-token\n",
    "            aligned_model_tokens[-1] += token  # so we merge the token with its preceding(s)\n",
    "            alignment_id -= 1\n",
    "        else:\n",
    "            aligned_model_tokens.append(token)\n",
    "        if aligned_model_tokens[-1] == original_tokens[\n",
    "            orig_idx]:  # A token was equal to an entire original word or a set of\n",
    "            orig_idx += 1  # sub-tokens was merged and matched an original word\n",
    "        alignment_ids.append(alignment_id)\n",
    "\n",
    "    if aligned_model_tokens != original_tokens:\n",
    "        raise Exception(\n",
    "            f'Failed to align tokens.\\nOriginal tokens: {original_tokens}\\nObtained alignment: {aligned_model_tokens}')\n",
    "    return alignment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba7cdc13-6ef8-4834-ae63-bf5e94da88c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subwords_alignment(dataset: list, tokenizer: AutoTokenizer, subword_prefix: str,\n",
    "                              lowercase: bool = False) -> dict:\n",
    "    sentence_alignment_dict = dict()\n",
    "\n",
    "    for sent_id, sentence in zip(dataset['id'], dataset['text']):\n",
    "        for tok_id, tok in enumerate(sentence):\n",
    "            if tok == '–':\n",
    "                sentence[tok_id] = '-'\n",
    "        if lowercase:\n",
    "            sentence = [word.lower() for word in sentence]\n",
    "        tokenized_sentence = tokenizer(sentence, is_split_into_words=True, return_tensors='pt')\n",
    "        input_ids = tokenized_sentence['input_ids'].tolist()[0]  # 0 because the batch_size is 1\n",
    "        model_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "        alignment_ids = align_to_original_words(model_tokens, sentence, subword_prefix, lowercase)\n",
    "        sentence_alignment_dict[sent_id] = {'model_input': tokenized_sentence, 'alignment_ids': alignment_ids}\n",
    "    return sentence_alignment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23931725-58ec-48d1-ba2b-11e037b9d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_alignment_dict = create_subwords_alignment(gaze_dataset, tokenizer, subword_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f67ffdbf-6b3a-42fb-905c-6e592e159d02",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AttentionMatrixExtractor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     attn_extractor \u001b[38;5;241m=\u001b[39m AltiContributionExtractor(model_name, layer, rollout, aggregation_method, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     attn_extractor \u001b[38;5;241m=\u001b[39m \u001b[43mAttentionMatrixExtractor\u001b[49m(model_name, layer, rollout, aggregation_method, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AttentionMatrixExtractor' is not defined"
     ]
    }
   ],
   "source": [
    "if method == 'valuezeroing':\n",
    "    attn_extractor = ValueZeroingContributionExtractor(model_name, layer, rollout, aggregation_method, 'cpu')\n",
    "elif method == 'alti':\n",
    "    attn_extractor = AltiContributionExtractor(model_name, layer, rollout, aggregation_method, 'cpu')\n",
    "else:\n",
    "    attn_extractor = AttentionMatrixExtractor(model_name, layer, rollout, aggregation_method, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af3faef-e0e7-4435-939a-b6f70cb5ac48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135318d4-5385-413f-be58-9f3e22fdb333",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
