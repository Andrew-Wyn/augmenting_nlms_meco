{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T08:32:15.832042599Z",
     "start_time": "2023-09-22T08:32:15.628110083Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from anm.modeling.multitask_xlm_roberta import XLMRobertaForMultiTaskSequenceClassification\n",
    "from anm.modeling.multitask_camembert import CamembertForMultiTaskSequenceClassification\n",
    "from transformers import TrainingArguments, AutoTokenizer, AutoConfig, get_scheduler\n",
    "from anm.gaze_dataloader.datacollator import MultiLabelDataCollatorWithPadding\n",
    "from anm.gaze_training.utils import  create_finetuning_optimizer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31b95501-de13-4a7d-9d7b-2f9efd3ab0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = {\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"lr\": 2e-5,#5e-6,\n",
    "    \"train_bs\": 8,\n",
    "    \"eval_bs\": 8,\n",
    "    \"n_epochs\": 1, #8,\n",
    "    \"seed\": 1234,\n",
    "    \"num_warmup_steps\": 0\n",
    "}\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, d=None):\n",
    "        if d is not None:\n",
    "            for key, value in d.items():\n",
    "                setattr(self, key, value)\n",
    "\n",
    "cf = Config(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ebc75f0-8d86-4738-9c4c-0d3d07b55891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_finetuned_model_path(model_cf, finetuned_models_dir, user_id):\n",
    "    model_str = 'xlm' if model_cf == 'cross_lingual' else 'camem'\n",
    "    finetuned_str = 'p' if model_cf.finetuned else 'np'    \n",
    "    model_dir = f'{finetuned_models_dir}/gaze_finetuning_it_{user_id}_{finetuned_str}_{model_str}'\n",
    "    for file_name in os.listdir(model_dir):\n",
    "        file_path = os.path.join(model_dir, file_name)\n",
    "        if file_name != 'tf_logs' and os.path.isdir(file_path):\n",
    "            if 'config.json' in os.listdir(file_path):\n",
    "                model_path = file_path\n",
    "            else:\n",
    "                inner_dir = os.listdir(file_path)[0]\n",
    "                model_path = os.path.join(file_path, inner_dir)\n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775718f8-275c-40a7-87ec-cbdd0c848073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config_with_tasks(model_name):\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    config.tasks = ['pos', 'neg']\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d51d3358-af28-484d-929a-3119a584abe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_cf, finetuned_models_dir=None, user_id=None):    \n",
    "    if model_cf.finetuned:\n",
    "        model_name = get_finetuned_model_path(model_cf, finetuned_models_dir, user_id)\n",
    "        config = get_config_with_tasks(model_name)\n",
    "        if model_cf.language_mode == 'cross_lingual':\n",
    "            model = XLMRobertaForMultiTaskSequenceClassification.from_pretrained(model_name, config=config, ignore_mismatched_sizes=True)\n",
    "        else:\n",
    "            model = CamembertForMultiTaskSequenceClassification.from_pretrained(model_name, config=config, ignore_mismatched_sizes=True)\n",
    "    else:\n",
    "        if model_cf.language_mode == 'cross_lingual':\n",
    "            model_name = 'xlm-roberta-base'\n",
    "            config = get_config_with_tasks(model_name)\n",
    "            if model_cf.pretrained:\n",
    "                model = XLMRobertaForMultiTaskSequenceClassification.from_pretrained(model_name, config=config)\n",
    "            else:\n",
    "                model = XLMRobertaForMultiTaskSequenceClassification(config=config)\n",
    "        else:\n",
    "            model_name = 'idb-ita/gilberto-uncased-from-camembert'\n",
    "            config = get_config_with_tasks(model_name)\n",
    "            if model_cf.pretrained:\n",
    "                model = CamembertForMultiTaskSequenceClassification.from_pretrained(model_name, config=config)\n",
    "            else:\n",
    "                model = CamembertForMultiTaskSequenceClassification(config=config)\n",
    "    return model, model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e22b3b5e-5a95-4d10-8fbf-0aebabb0aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cf = {\n",
    "    'language_mode': 'cross_lingual',\n",
    "    'pretrained': True,\n",
    "    'finetuned': False\n",
    "}\n",
    "\n",
    "model_cf = Config(model_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6e2053e-9118-42d7-b668-ceef87b443d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForMultiTaskSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifiers.neg.dense.bias', 'classifiers.pos.dense.weight', 'classifiers.pos.out_proj.bias', 'classifiers.pos.dense.bias', 'classifiers.pos.out_proj.weight', 'classifiers.neg.out_proj.bias', 'classifiers.neg.out_proj.weight', 'classifiers.neg.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "finetuned_models_dir = '/home/lmoroni/__workdir/augmenting_nlms_meco/output'\n",
    "# for user_id  in [1, 26, 38, 43, 44]:\n",
    "    # model = load_model(model_cf, finetuned_models_dir=finetuned_models_dir, user_id=user_id)\n",
    "model, model_name = load_model(model_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62ee53e082417c54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T08:32:27.526134051Z",
     "start_time": "2023-09-22T08:32:27.514960046Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_dir = '../augmenting_nlms_meco_data/sentiment/it_sentipolc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d3f7737953e40f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T08:33:55.673492478Z",
     "start_time": "2023-09-22T08:33:55.598121334Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sentipolc_files = {\n",
    "    'train': [os.path.join(dataset_dir, file_name) for file_name in os.listdir(dataset_dir) if 'training_set' in file_name][0],\n",
    "    'test': [os.path.join(dataset_dir, file_name) for file_name in os.listdir(dataset_dir) if 'test_set' in file_name][0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fc2c8d0daaefdd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-22T08:34:00.057501469Z",
     "start_time": "2023-09-22T08:34:00.055380928Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': '../augmenting_nlms_meco_data/sentiment/it_sentipolc/training_set_sentipolc16.csv',\n",
       " 'test': '../augmenting_nlms_meco_data/sentiment/it_sentipolc/test_set_sentipolc16_gold2000.csv'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentipolc_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11b1ae83-81db-4216-b264-30bdaa6f8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_from_faulty_csv(src_path):\n",
    "    dataset_dict = {'text': [], 'label_pos': [], 'label_neg':[]}\n",
    "    with open(src_path) as src_file:\n",
    "        csv_reader = csv.reader(src_file, delimiter=',', quotechar='\"')\n",
    "        print('')\n",
    "        for row in csv_reader:\n",
    "            if row[0] == 'idtwitter':\n",
    "                continue\n",
    "            if len(row) != 9:\n",
    "                cut_row = row[:9]\n",
    "                cut_row[8] += ',' + ', '.join(row[9:])\n",
    "                row = cut_row\n",
    "            dataset_dict['text'].append(row[8])\n",
    "            dataset_dict['label_pos'].append(int(row[2]))\n",
    "            dataset_dict['label_neg'].append(int(row[3]))\n",
    "    return Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7844f58-2f31-4c8b-9f9a-1e88eedecfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = create_dataset_from_faulty_csv(sentipolc_files['train'])\n",
    "test_dataset = create_dataset_from_faulty_csv(sentipolc_files['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7985884-03ae-4b13-b20c-57aca93c38b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text', 'label_pos', 'label_neg'],\n",
       "     num_rows: 7410\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['text', 'label_pos', 'label_neg'],\n",
       "     num_rows: 1998\n",
       " }))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfb6cb27-02d8-45f6-8e26-07309f1ef973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_name = 'xlm-roberta-base' if model_cf.language_mode == 'cross_lingual' else 'idb-ita/gilberto-uncased-from-camembert'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df17708a-93b3-4861-ae81-f02c17a94155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    result = tokenizer(examples[\"text\"], truncation=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcc023c2-3edf-423e-9351-11640bb2e14c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c2218002dbf46749af5cc84f9788c50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7410 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3479028b8afc47b5ad7aa5d0c8c91852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1998 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_ds = train_dataset.map(preprocess_function, batched=True, remove_columns=['text'])\n",
    "tokenized_test_ds = test_dataset.map(preprocess_function, batched=True, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cea2779-7552-4830-bdc7-97255d38f64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label_pos', 'label_neg', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 7410\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c777d1f-a033-432f-b037-7c4f4c668648",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = MultiLabelDataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9594089e-d17b-4eea-a11a-2cb775316681",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(tokenized_train_ds, shuffle=True, collate_fn=data_collator, batch_size=cf.train_bs)\n",
    "eval_dataloader = DataLoader(tokenized_test_ds, collate_fn=data_collator, batch_size=cf.eval_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c2656c2-9009-47c5-9e63-fdc3528cbc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": cf.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=cf.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "603ef0ff-efd3-46ae-bf0d-e97291873c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_steps = cf.n_epochs * math.ceil(len(train_dataloader))\n",
    "lr_scheduler = get_scheduler(name='linear', \n",
    "                             optimizer=optimizer, \n",
    "                             num_warmup_steps=cf.num_warmup_steps,\n",
    "                             num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fefed964-ab08-4a25-bd92-6753f68dc090",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa674f3a-07e7-43b6-88e1-d5188c8e3826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7c7ff90-60b4-420f-8145-f5621b2991e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "f1 = evaluate.load('f1')\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25b2fb24-2843-4c78-a9b1-e21f911227f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "345a0299c1cd459c97ebb28a17bd73cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/927 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.4195045530796051\n",
      "Eval accuracy {'f1': 0.5749506903353058}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(1, cf.n_epochs + 1):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = {\n",
    "            \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "            \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "            \"labels\": {k: v.to(device) for k, v in batch[\"labels\"].items()}\n",
    "        }\n",
    "\n",
    "        model_output = model(**batch)\n",
    "        loss = model_output.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=cf.max_grad_norm)\n",
    "\n",
    "        \n",
    "    print(\"Loss\", loss.item())\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            batch = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                \"labels\": {k: v.to(device) for k, v in batch[\"labels\"].items()}\n",
    "            }\n",
    "\n",
    "            model_output = model(**batch)\n",
    "\n",
    "            for task in batch['labels']:\n",
    "                predictions = model_output.logits[task].argmax(dim=-1)\n",
    "                references = batch['labels'][task]\n",
    "                f1.add_batch(predictions=predictions, references=references)\n",
    "\n",
    "    eval_accuracy = f1.compute()\n",
    "    print(f'Eval accuracy {eval_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d54efbcb-86e8-485f-a70f-95ac6f1b5fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, split, all_metrics, pos_metrics, neg_metrics):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {\n",
    "                \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "                \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "                \"labels\": {k: v.to(device) for k, v in batch[\"labels\"].items()}\n",
    "            }\n",
    "    \n",
    "            model_output = model(**batch)\n",
    "    \n",
    "            for task in batch['labels']:\n",
    "                predictions = model_output.logits[task].argmax(dim=-1)\n",
    "                references = batch['labels'][task]\n",
    "                # for metric in metrics:\n",
    "                all_metrics.add_batch(predictions=predictions, references=references)\n",
    "                if task  == 'pos':\n",
    "                    pos_metrics.add_batch(predictions=predictions, references=references)\n",
    "                elif task == 'neg':\n",
    "                    neg_metrics.add_batch(predictions=predictions, references=references)\n",
    "\n",
    "    \n",
    "    \n",
    "    all_res = all_metrics.compute()\n",
    "    pos_res = pos_metrics.compute()\n",
    "    neg_res = neg_metrics.compute()\n",
    "\n",
    "    res_dict = {}\n",
    "    \n",
    "    for metric in all_res:\n",
    "        res_dict[f'{split}_{metric}'] = all_res[metric]  \n",
    "    for metric in pos_res:\n",
    "        res_dict[f'{split}_pos_{metric}'] = pos_res[metric]\n",
    "    for metric in neg_res:\n",
    "        res_dict[f'{split}_neg_{metric}'] = neg_res[metric]\n",
    "        \n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93f4cdf1-0fe3-4c2c-b750-f1593dec8a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_metrics():\n",
    "    return evaluate.combine([\n",
    "        evaluate.load(\"accuracy\"),\n",
    "        evaluate.load(\"f1\"),\n",
    "        evaluate.load(\"precision\", average='binary'),\n",
    "        evaluate.load(\"recall\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8538e5ad-f6c7-403b-a062-cfe1a5696a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_out_path(out_dir, model_cf, user_id=None):\n",
    "    model_str = 'xlm' if model_cf.language_mode == 'cross_lingual' else 'camem'\n",
    "    pretrained = 'p' if model_cf.pretrained else 'np'\n",
    "    finetuned = f'f_it{user_id}' if model_cf.finetuned else 'nf'\n",
    "    if not model_cf.finetuned and model_cf.language_mode == 'cross_lingual':\n",
    "        model_str += '_it'\n",
    "    return os.path.join(out_dir, f'{model_str}_{pretrained}_{finetuned}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80abde77-4c2c-4cf5-a42d-cca77c445522",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = f'../output/sentipolc'\n",
    "out_path = get_out_path(model_save_dir, model_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e284fe9-5b96-48e8-b8c8-3434d0df51a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c90de917-0160-4d50-9eb3-b953deb0c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res = evaluate_model(model, train_dataloader, 'train', get_eval_metrics(), get_eval_metrics(), get_eval_metrics())\n",
    "test_res = evaluate_model(model, eval_dataloader, 'test', get_eval_metrics(), get_eval_metrics(), get_eval_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1aff01c7-76a7-43b9-b18e-30ed0a8a4dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_accuracy': 0.7898110661268556,\n",
       " 'train_f1': 0.688219397457712,\n",
       " 'train_precision': 0.6935646560419608,\n",
       " 'train_recall': 0.6829558998808105,\n",
       " 'train_pos_accuracy': 0.8005398110661268,\n",
       " 'train_pos_f1': 0.63160518444666,\n",
       " 'train_pos_precision': 0.6460989291177971,\n",
       " 'train_pos_recall': 0.6177474402730375,\n",
       " 'train_neg_accuracy': 0.7790823211875844,\n",
       " 'train_neg_f1': 0.7262083960528516,\n",
       " 'train_neg_precision': 0.7246328437917223,\n",
       " 'train_neg_recall': 0.7277908146161582}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30dd0d1-0581-42e3-b2be-4a7861b69ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = train_res|test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8700bce4-6256-42ab-879c-bc12dcdb247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce9016b-0744-4a38-a394-1e26c1a8292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "metrics_out_path = os.path.join(out_path, 'all_results.json')\n",
    "with open(metrics_out_path, 'w+') as out_file:\n",
    "    json.dump(train_res | test_res, out_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
